{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "In order to train the model we we were use the MNIST data set. \n",
    "The first step is to load the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    f = gzip.open(path)\n",
    "    training_data, validation_data, test_data = pickle.load(f,encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "training_data, validation_data, test_data = load_data('mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing and batch generation\n",
    "\n",
    "The MNIST dataset requires minimal processing. We will transform the labels into one hot vectors for convienient comparisons with the model output. We will also define a function that produces random batches of training data, this will be important later for implementing the stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    r = np.zeros((10,1))\n",
    "    r[j] = 1\n",
    "    return r\n",
    "\n",
    "def get_batch(input_data, output_data, batch_size):\n",
    "    \n",
    "    n = len(input_data)\n",
    "    idx = np.random.choice(n, batch_size)\n",
    "    input_batch = [np.reshape(input_data[i], (784, 1)) for i in idx]\n",
    "    output_batch = [one_hot(output_data[i]) for i in idx]\n",
    "    \n",
    "    x_batch = np.reshape(input_batch, [batch_size, 784])\n",
    "    y_batch = np.reshape(output_batch, [batch_size, 10])\n",
    "    \n",
    "    return x_batch, y_batch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model definition\n",
    "Now we are ready to start building the neural network used for digit recognition. We will first define the model class that will hold information about layers arrangement, weights and biases. The weights and biases will be initialized with random numbers on model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers_count = len(layers)\n",
    "        self.layers = layers\n",
    "        self.biases = [np.random.randn(1, layer) for layer in layers[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define forward propagation and backprogagation we will also need some activation function. The sigmoid function was chosen for this purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to define a cost function. We will use mean square error function. ITs derivative will be used for the gradient computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_prime(output_activations, y):\n",
    "    return (output_activations-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward pass\n",
    "\n",
    "Now we are ready to define the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(model, x):\n",
    "    \n",
    "    for w, b in zip(model.weights, model.biases):\n",
    "        z = np.dot(x, w.T) + b\n",
    "        x = sigmoid(z)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training definition\n",
    "\n",
    "In order to obtain some meanigful results, we first need to train the model. Here we define a single training step. The function is called with a batch of inputs and labels. Then it uses backpropagation to compute the gradient and update the weights and biases accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_batch, output_batch, learning_rate):\n",
    "            \n",
    "    delta_nabla_b, delta_nabla_w = backprop(model, input_batch, output_batch)\n",
    "        \n",
    "    model.weights = [w-(learning_rate/len(input_batch))*nw\n",
    "        for w, nw in zip(model.weights, delta_nabla_w)]\n",
    "    model.biases = [b-(learning_rate/len(input_batch))*nb\n",
    "        for b, nb in zip(model.biases, delta_nabla_b)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of the trainig_step function is the backpropagation algorithm, which is implemented by the backprop function.\n",
    "First we initialize the forward pass trough the network saving the resulting activations. Then in the reverse order we compute the partial derivates and store them in the gradient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(model, x, y):\n",
    "        \n",
    "    nabla_b = [np.zeros(b.shape) for b in model.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in model.weights]\n",
    "    activation = x\n",
    "    activations = [x]\n",
    "    zs = []\n",
    "    \n",
    "    \n",
    "    #Forward pass    \n",
    "    for w, b in zip(model.weights, model.biases):\n",
    "        z = np.dot(activation, w.T) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "            \n",
    "    ##Backward pass        \n",
    "    delta = cost_prime(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = np.sum(delta, axis=0)\n",
    "    nabla_w[-1] = np.dot(delta.T, activations[-2])\n",
    "    \n",
    "    for l in range(2, model.layers_count):\n",
    "        z = zs[-l]\n",
    "        delta = np.dot(delta, model.weights[-l+1]) * sigmoid_prime(z)\n",
    "        nabla_b[-l] = np.sum(delta, axis=0)\n",
    "        nabla_w[-l] = np.dot(delta.T, activations[-l-1])\n",
    "        \n",
    "    return (nabla_b, nabla_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now we are ready to start training our model. We need to specify the batch size, number of iterations and the learning rate, and launch the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20000/20000 [00:41<00:00, 478.75it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "iterations = 20000\n",
    "learning_rate = 2.0\n",
    "\n",
    "model = Model([784,300,100,10])\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear()\n",
    "for _ in tqdm(range(iterations)):\n",
    "    x, y = get_batch(training_data[0], training_data[1], batch_size)\n",
    "    training_step(model,x,y,learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "After the training is completed, we can use the test set in order to check how good our model has become at recognizing digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model classified correctly 9441 out of 10000 test examples\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [np.reshape(x, (1, 784)) for x in test_data[0]]\n",
    "test_outputs = test_data[1]\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(test_inputs)):\n",
    "    x = call(model, test_inputs[i])\n",
    "    predicted_id = np.argmax(x)\n",
    "    if predicted_id == test_outputs[i]:\n",
    "        counter+=1\n",
    "print(f'The model classified correctly {counter} out of {len(test_inputs)} test examples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
