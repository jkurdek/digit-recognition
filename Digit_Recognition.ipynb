{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "In order to train the model we we were use the MNIST data set. \n",
    "The first step is to load the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    f = gzip.open(path)\n",
    "    training_data, validation_data, test_data = pickle.load(f,encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "training_data, validation_data, test_data = load_data('mnist.pkl.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing\n",
    "The MNIST dataset requires minimal processing. We will transform the labels into one hot vectors for convienient comparisons with the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    r = np.zeros((10,1))\n",
    "    r[j] = 1\n",
    "    return r\n",
    "\n",
    "def process_data(data, one_hot_output=False):\n",
    "    input_data = [np.reshape(i, (1, 784)) for i in data[0]]\n",
    "    \n",
    "    if one_hot_output:\n",
    "        output_data = [one_hot(l) for l in data[1]]\n",
    "    else:\n",
    "        output_data = [l for l in data[1]]\n",
    "    \n",
    "    return (input_data, output_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch generation\n",
    "\n",
    "We will also define a function that produces random batches of training data, this will be important later for implementing the stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(input_data, output_data, batch_size):\n",
    "    \n",
    "    n = len(input_data)\n",
    "    idx = np.random.choice(n, batch_size)\n",
    "    input_batch = [input_data[i] for i in idx]\n",
    "    output_batch = [output_data[i] for i in idx]\n",
    "    \n",
    "    x_batch = np.reshape(input_batch, [batch_size, 784])\n",
    "    y_batch = np.reshape(output_batch, [batch_size, 10])\n",
    "    \n",
    "    return x_batch, y_batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define a cost functions. For experimentation we will use both MSE and Cross Entropy cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(object):\n",
    "    @staticmethod\n",
    "    def delta(output_activations, y, z):\n",
    "        \"\"\"\n",
    "        Returns derivative of cost function with respect to the activation \n",
    "        multiplied by the derivative of the activation with respect to z(z = w*aL-1 + b).\n",
    "        delta = dC/daL * daL/dzL\n",
    "        \"\"\"\n",
    "        return (output_activations-y) * sigmoid_prime(z)\n",
    "    \n",
    "class CrossEntropy:\n",
    "    @staticmethod\n",
    "    def delta(output_activations, y, z):\n",
    "        \"\"\"\n",
    "        Returns derivative of cost function with respect to the activation \n",
    "        multiplied by the derivative of the activation with respect to z(z = w*aL-1 + b).\n",
    "        delta = dC/daL * daL/dzL\n",
    "        \"\"\"\n",
    "        return (output_activations-y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start building the neural network used for digit recognition. We will first define the model class that will hold information about the cost function, layers arrangement, weights and biases. The weights and biases will be initialized with random numbers on model creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, layers, cost=CrossEntropy):\n",
    "        self.layers_count = len(layers)\n",
    "        self.layers = layers\n",
    "        self.cost = cost\n",
    "        self.biases = [np.random.randn(1, layer) for layer in layers[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define forward propagation and backprogagation we will also need some activation function. The sigmoid function was chosen for this purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward pass\n",
    "\n",
    "Now we are ready to define the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(model, x):\n",
    "    \n",
    "    for w, b in zip(model.weights, model.biases):\n",
    "        z = np.dot(x, w.T) + b\n",
    "        x = sigmoid(z)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training definition\n",
    "\n",
    "In order to obtain some meanigful results, we first need to train the model. Here we define a single training step. The function is called with a batch of inputs and labels. Then it uses backpropagation to compute the gradient and update the weights and biases accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_batch, output_batch, learning_rate):\n",
    "            \n",
    "    delta_nabla_b, delta_nabla_w = backprop(model, input_batch, output_batch)\n",
    "        \n",
    "    model.weights = [w-(learning_rate/len(input_batch))*nw\n",
    "        for w, nw in zip(model.weights, delta_nabla_w)]\n",
    "    model.biases = [b-(learning_rate/len(input_batch))*nb\n",
    "        for b, nb in zip(model.biases, delta_nabla_b)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of the trainig_step function is the backpropagation algorithm, which is implemented by the backprop function.\n",
    "First we initialize the forward pass trough the network saving the resulting activations. Then in the reverse order we compute the partial derivates and store them in the gradient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(model, x, y):\n",
    "        \n",
    "    nabla_b = [np.zeros(b.shape) for b in model.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in model.weights]\n",
    "    activation = x\n",
    "    activations = [x]\n",
    "    zs = []\n",
    "    \n",
    "    \n",
    "    #Forward pass    \n",
    "    for w, b in zip(model.weights, model.biases):\n",
    "        z = np.dot(activation, w.T) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "            \n",
    "    #Backward pass        \n",
    "    delta = model.cost.delta(activations[-1], y, zs[-1])\n",
    "    nabla_b[-1] = np.sum(delta, axis=0)\n",
    "    nabla_w[-1] = np.dot(delta.T, activations[-2])\n",
    "    \n",
    "    for l in range(2, model.layers_count):\n",
    "        z = zs[-l]\n",
    "        delta = np.dot(delta, model.weights[-l+1]) * sigmoid_prime(z)\n",
    "        nabla_b[-l] = np.sum(delta, axis=0)\n",
    "        nabla_w[-l] = np.dot(delta.T, activations[-l-1])\n",
    "        \n",
    "    return (nabla_b, nabla_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation definition\n",
    "\n",
    "In order to test our solution we will define a simple function that check model's output against the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_inputs, test_outputs):\n",
    "    n = len(test_inputs)\n",
    "    return sum([test_outputs[i] == np.argmax(call(model, test_inputs[i])) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training\n",
    "\n",
    "Now we are ready to start training our model. We need to specify the batch size, number of iterations and the learning rate, and launch the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████▋                                 | 55000/100000 [02:16<01:51, 403.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped early\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "max_iterations = 100000\n",
    "learning_rate = 2.0\n",
    "epoch = 5000\n",
    "\n",
    "prev_correct = 0\n",
    "break_counter = 0\n",
    "max_decrease = 2\n",
    "\n",
    "model = Model([784,300,100,10])\n",
    "training_inputs, training_outputs = process_data(training_data, one_hot_output=True)\n",
    "validatation_inputs, validation_outputs = process_data(validation_data, one_hot_output=False)\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear()\n",
    "for i in tqdm(range(max_iterations)):\n",
    "    x, y = get_batch(training_inputs, training_outputs, batch_size)\n",
    "    training_step(model, x, y, learning_rate)\n",
    "\n",
    "    #Early stopping, if the accuracy on validation set is decrasing stop the training\n",
    "    if i % epoch==0:\n",
    "        correct = evaluate(model, validatation_inputs, validation_outputs)\n",
    "        if correct < prev_correct:\n",
    "            break_counter+=1\n",
    "        else:\n",
    "            break_counter = 0\n",
    "        if break_counter == max_decrease:\n",
    "            print(\"Stopped early\")\n",
    "            break;\n",
    "        prev_correct = correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing\n",
    "After the training is completed, we can use the test set in order to check how good our model has become at recognizing digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model classified correctly 9717 out of 10000 test examples\n"
     ]
    }
   ],
   "source": [
    "test_inputs, test_outputs = process_data(test_data, one_hot_output=False)\n",
    "correct = evaluate(model, test_inputs, test_outputs)\n",
    "\n",
    "\n",
    "print(f'The model classified correctly {correct} out of {len(test_inputs)} test examples')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
